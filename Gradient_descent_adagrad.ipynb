{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmqQRnrcWzsZjXCMSGv6KM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BDOolhpQ8cqE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data[50:]\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "y = iris.target[50:]\n",
        "y[:50] = 0\n",
        "y[50:] = 1"
      ],
      "metadata": {
        "id": "bx7YR3F-8tdY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "Логистическая регрессия - вероятностная бинарная модель, которая является аналогом линейного классификатора. Считаем взвешенную сумму весов и входящих данных. Передаём её логистической функции, которая выдаёт вероятностную оценку принадлежности к классу.  \n",
        "Оцениваем работу модели с помощью логистической функции потерь.  \n",
        "Улучшаем качество модели с помощью итеративного алгоритма градиентного спуска."
      ],
      "metadata": {
        "id": "IiSIgkkY-mqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_w):\n",
        "    return 1 / (1 + np.exp(-x_w))"
      ],
      "metadata": {
        "id": "s6Kj6C4Q-foK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = X_b.shape[1]"
      ],
      "metadata": {
        "id": "VXH9u00v_oVc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Градиентный спуск  "
      ],
      "metadata": {
        "id": "fUhiBpW_p7nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_pred, y_true):\n",
        "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
      ],
      "metadata": {
        "id": "aj7JN3Xjp6t-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.random.randn(d, 1) # рандомная инициализация весов\n",
        "epochs = 10000\n",
        "TMP = 10 # константное значение \n",
        "for epoch in range(1, epochs+1):\n",
        "    grad = np.zeros((d, 1))\n",
        "    glob_error = 0\n",
        "    eta = TMP / epoch # шаг обучения\n",
        "    for point, target in zip(X_b, y): # можно реализовать обучение векторно\n",
        "        prob = logistic_regression(point.dot(weights))\n",
        "        glob_error += log_loss(prob, target)\n",
        "        grad += (prob - target) * point.reshape((-1, 1)) / X.shape[0]\n",
        "    print(\"ERROR: {}\".format(glob_error / X.shape[0]))\n",
        "    weights -= eta * grad"
      ],
      "metadata": {
        "id": "N1SI4CIMtmGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adagrad\n",
        "Отличие от классического градиентного спуска в том, что веса меняются относительно каждой фичи. Использование данного оптимизатора полезно, когда фичи имеют разные масштабы.  \n",
        "Например спам фильтр. Частоты появления различных слов могут варьироваться."
      ],
      "metadata": {
        "id": "_kzm2XhGQgHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_weights = np.random.randn(d, 1)\n",
        "epochs = 100\n",
        "eta = 0.1\n",
        "for epoch in range(1, epochs+1):\n",
        "    grad = np.zeros((d, 1))\n",
        "    z = np.zeros((d, 1))\n",
        "    glob_error = 0\n",
        "    for point, target in zip(X_b, y):\n",
        "        prob = logistic_regression(point.dot(weights))\n",
        "        glob_error += log_loss(prob, target)\n",
        "        grad += (prob - target) * point.reshape((-1, 1)) / X.shape[0]\n",
        "        z += grad ** 2\n",
        "    print(\"ERROR: {}\".format(glob_error / X.shape[0]))\n",
        "    ada_weights -= eta * grad / np.sqrt(z + 1e-7)"
      ],
      "metadata": {
        "id": "APYm2j4VzW4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}