# ML_NN_Edu

### cal_housing_prices - задача прогнозирования цен на жильё Калифорнийских домов
### mnist_edu - задача классификации на наборе данных mnist
### Linear_models - Линейная регрессия, Полиноминальная регрессия, Градиентный спуск, Стохастический градиентный спуск, Гребневая регрессия, Лассо регрессия, Логистическая регрессия (sigmoid + logloss), Многозначная регрессия (softmax + cross entropy)
### KNN - knn классификатор 
 ИДЕЯ: близкие точки имеют одинаковые лейблы
### Perceptron - бинарная классификация сгенерированных точек, бинарная классификация '7' и '0'
 ИДЕЯ: Если данные линейно разделимые, то существует гиперплоскость, которая разделит данные
### Naive_Bayes - Вероятностная модель использующая MLE, MAP идеи поиска параметров. Задачи с категориальными, непрерывными и мультиномиальными фичами
 ИДЕЯ: В отличие от Оптимального Байесовского классификатора делается предположение о том, что все фичи независимы друг от друга.
 h(x) = argmaxP(y|x) = argmax(P(x1|y) * P(x2|y) * ... * P(xn|y) * P(y))  
 ### Gradient_descent_adagrad - Оптимизационные алгоритмы.  
 ### SVM - бинарная классификация на сгенерированном датасете
 ИДЕЯ: Алгоритм SVM находит такую разделяющую гиперплоскость, расстояние от которой до ближайших экземпляров будет максимально возможным.
 ### kernel_methods
 ### KD Tree - алгоритм поиска ближайших соседей при помощи дерева
 ИДЕЯ: Идея заключается в предварительном рекурсивном разграничивании исходных данных по оси(не плохо брать фичу с самым большим разбросом) с помощью бинарного дерева и поиска ближайших соседей на листах (KNN).
 При обратном ходе рекурсии проверяется расстояние от x_test до границы, если оно меньше чем расстояние от x_test до ближайшего соседа , то рекурсией переходим на другую ветку и алгоритм находит соседей на другой сторое, иначе делаем обратный шаг. (Не работает на данных с большими размерностями(даже если спроецировать данные на m размерностях m<d), хорошо работает для 2-3, может не плохо работать для d <= 10)
